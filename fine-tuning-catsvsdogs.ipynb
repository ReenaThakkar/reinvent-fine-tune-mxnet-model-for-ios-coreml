{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning a CNN with MXNet: Cats vs Dogs (Kaggle Redux)\n",
    "\n",
    "In this tutorial we'll learn how to build a model to classifiy if an image is a cat or a dog. We'll use a pre-trained [imagenet](http://www.image-net.org/) model from the MXNet [model zoo](http://data.mxnet.io/models/). For practical problems we may not have a large dataset, hence its difficult to train these generalized models. However we can take advantage of models that are pre-trained on large dataset like imagenet where in the model has already learnt a lot of the image features. \n",
    "\n",
    "For example, the ImageNet dataset, the default academic benchmark, which contains 1M million images, 1000 each from 1000 separate classes. The ImageNet dataset categorically changed what was possible in computer vision. It turns out some things are possible (these days, even easy) on gigantic datasets, that simply aren't with smaller datasets. In fact, we don't know of any technique that can comparably powerful model on a similar photograph dataset but containing only, say, 10k images.\n",
    "\n",
    "And that's a problem. Because however impressive the results of CNNs on ImageNet may be, most people aren't interested in ImageNet itself. They're interested in their own problems. Recognize people based on pictures of their faces. Distinguish between photographs of $10$ different types of corral on the ocean floor. Usually when individuals (and not Amazon, Google, or inter-institutional big science initiatives) are interested in solving a computer vision problem, they come to the table with modestly sized datasets. A few hundred examples may be common and a few thousand examples may be as much as you can reasonably ask for.\n",
    "\n",
    "So one natural question emerges.\n",
    "Can we somehow use the powerful models trained on millions of examples for one dataset,\n",
    "and apply them to improve performance on a new problem \n",
    "with a much smaller dataset?\n",
    "This kind of problem (learning on source dataset, bringing knowledge to target dataset),\n",
    "is appropriately called *transfer learning*. \n",
    "Fortunately, we have some effective tools for solving this problem.\n",
    "\n",
    "For deep neural networks, the most popular approach is called finetuning\n",
    "and the idea is both simple and effective:\n",
    "\n",
    "* Train a neural network on the source task $S$.\n",
    "* Decapitate it, replacing it's output layer appropriate to target task $T$.\n",
    "* Initialize the weights on the new output layer randomly, keeping all other (pretrained) weights the same.\n",
    "* Begin training on the new dataset.\n",
    "\n",
    "This might be clearer if we visualize the algorithm: \n",
    "\n",
    "![](img/fine-tune.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up a deep learning environment with AWS Deep Learning AMI for MXNet\n",
    "\n",
    "In this tutorial, we are going to use [Deep Learning AMI](https://aws.amazon.com/marketplace/pp/B06VSPXKDX). The Deep Learning AMI is a base Amazon Linux image provided by Amazon Web Services for use on Amazon Elastic Compute Cloud (Amazon EC2).It is designed to provide a stable, secure, and high performance execution environment for deep learning applications running on Amazon EC2. It includes popular deep learning frameworks, including MXNet.\n",
    "\n",
    "For setting up an Deep Learning environment on AWS using Deep Learning AMI, please read [this post on AWS AI Blog](https://aws.amazon.com/blogs/ai/the-aws-deep-learning-ami-now-with-ubuntu/) for detailed instruction. \n",
    "\n",
    "Or you can choose to [install MXNet](http://mxnet.io/get_started/install.html) to your own machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    " - MXNet (0.11.0 or higher)\n",
    " - Python 2.7 or higher\n",
    " - im2rec.py (clone https://github.com/dmlc/mxnet) \n",
    " - [recommended] Amazon EC2 instance with GPU (p2.* family) with [Deep Learning AMI](https://bit.ly/deepami)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "sudo wget http://files.fast.ai/data/dogscats.zip\n",
    "sudo unzip dogscats.zip\n",
    "sudo rm dogscats.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the function which returns the data iterators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list for training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python ../src/mxnet/tools/im2rec.py --list True --recursive True cats_dogs_train dogscats/train\n",
    "python ../src/mxnet/tools/im2rec.py --list True --recursive True cats_dogs_val dogscats/valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the images in to MXNet RecordIO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python ../src/mxnet/tools/im2rec.py --resize 224 --quality 90 --num-thread 16 cats_dogs_train.lst dogscats/train\n",
    "\n",
    "python ../src/mxnet/tools/im2rec.py --resize 224 --quality 90 --num-thread 16 cats_dogs_val.lst dogscats/valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import os, urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Iterators for cats vs dogs dataset\n",
    "def get_iterators(batch_size, data_shape=(3, 224, 224)):\n",
    "    train = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = './cats_dogs_train.rec', \n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = data_shape,\n",
    "        shuffle             = True,\n",
    "        rand_crop           = True,\n",
    "        rand_mirror         = True)\n",
    "    val = mx.io.ImageRecordIter(\n",
    "        path_imgrec         = './cats_dogs_val.rec',\n",
    "        data_name           = 'data',\n",
    "        label_name          = 'softmax_label',\n",
    "        batch_size          = batch_size,\n",
    "        data_shape          = data_shape,\n",
    "        rand_crop           = False,\n",
    "        rand_mirror         = False)\n",
    "    return (train, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dowload pre-trained model from the model zoo (Squeezenet_v1.1)\n",
    "\n",
    "SqueezeNet is a state-of-the-art inception model based Deep Neural Network that find a good balance between classification accuracy and memory usage. Taking our computing time and resource budget into account, SqueezeNet is also the most reasonable model for us to work on. \n",
    "\n",
    "The smaller CNN architectures of SqueezeNet offer at least three advantages: (1) Smaller CNNs require less communication across servers during distributed training. (2) Smaller CNNs require less bandwidth to export a new model from the cloud to an autonomous car and make your iOS app overall size smaller. (3) Smaller CNNs are more feasible to deploy on mobile devices with limited memory. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques, we are able to compress SqueezeNet to less\n",
    "than 0.5MB (510× smaller than AlexNet).\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1000/1*RaSehomyb8PZbpg2xnTamQ.png \"Title\")\n",
    "\n",
    "    Note: If load_checkpoint reports error, we can remove the downloaded files and try get_model again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def download(url):\n",
    "    filename = url.split(\"/\")[-1]\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.urlretrieve(url, filename)\n",
    "        \n",
    "def get_model(prefix, epoch):\n",
    "    download(prefix+'-symbol.json')\n",
    "    download(prefix+'-%04d.params' % (epoch,))\n",
    "\n",
    "model_name = 'squeezenet_v1.1'\n",
    "    \n",
    "get_model('http://data.mxnet.io/models/imagenet/squeezenet/'+model_name, 0)\n",
    "sym, arg_params, aux_params = mx.model.load_checkpoint(model_name, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the model\n",
    "\n",
    "\n",
    "To fine-tune a network, we must first replace the last fully-connected layer with a new one that outputs the desired number of classes. We initialize its weights randomly. Then we continue training as normal. Sometimes it’s common use a smaller learning rate based on the intuition that we may already be close to a good result.\n",
    "\n",
    "We first define a function which replaces the the last fully-connected layer for a given network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fine_tune_model(symbol, arg_params, num_classes, layer_name='flatten_0'):\n",
    "    \"\"\"\n",
    "    symbol: the pre-trained network symbol\n",
    "    arg_params: the argument parameters of the pre-trained model\n",
    "    num_classes: the number of classes for the fine-tune datasets\n",
    "    layer_name: the layer name before the last fully-connected layer\n",
    "    \"\"\"\n",
    "    all_layers = sym.get_internals()\n",
    "    #print(all_layers)\n",
    "    net = all_layers[layer_name+'_output']\n",
    "    net = mx.symbol.FullyConnected(data=net, num_hidden=num_classes, name='fc1')\n",
    "    net = mx.symbol.SoftmaxOutput(data=net, name='softmax')\n",
    "    new_args = dict({k:arg_params[k] for k in arg_params if 'fc1' not in k})\n",
    "    return (net, new_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model\n",
    "\n",
    "We now define a fit function that creates an MXNet module instance that we'll bind the data and symbols to. \n",
    "\n",
    "init_params is called to randomly initialize parameters\n",
    "\n",
    "set_params is called to replace all parameters except for the last fully-connected layer with pre-trained model.\n",
    "\n",
    "#### Note: change mx.gpu to mx.cpu to run training on CPU (much slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "head = '%(asctime)-15s %(message)s'\n",
    "logging.basicConfig(level=logging.DEBUG, format=head)\n",
    "\n",
    "def fit(symbol, arg_params, aux_params, train, val, batch_size, num_gpus=0, num_epoch=1, learning_rate=0.009):\n",
    "    devs = [mx.gpu(i) for i in range(num_gpus)] if num_gpus > 0 else [mx.cpu()]\n",
    "    mod = mx.mod.Module(symbol=new_sym, context=devs)\n",
    "    mod.bind(data_shapes=train.provide_data, label_shapes=train.provide_label)\n",
    "    mod.init_params(initializer=mx.init.Xavier(rnd_type='gaussian', factor_type=\"in\", magnitude=2))\n",
    "    mod.set_params(new_args, aux_params, allow_missing=True)\n",
    "    mod.fit(train, val, \n",
    "        num_epoch=num_epoch,\n",
    "        batch_end_callback = mx.callback.Speedometer(batch_size, 10),        \n",
    "        kvstore='device',\n",
    "        optimizer='adam',\n",
    "        optimizer_params={'learning_rate': learning_rate},\n",
    "        eval_metric='acc')\n",
    "    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the helper functions setup, we can start training.\n",
    "Its recommended that you train on a GPU instance, preferably p2.* family. In this example we assume an AWS EC2 p2.xlarge, which has one NVIDIA K80 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "demo = True\n",
    "\n",
    "if demo:\n",
    "    epochs = 5\n",
    "    learning_rate = 1e-4\n",
    "else:\n",
    "    epochs = 40\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "num_classes = 2 # This is binary classification (dogs vs cat)\n",
    "batch_per_gpu = 128\n",
    "num_gpus = 1 #replace with 0 if train with CPU\n",
    "\n",
    "(new_sym, new_args) = get_fine_tune_model(sym, arg_params, num_classes, 'flatten')\n",
    "\n",
    "batch_size = batch_per_gpu * num_gpus\n",
    "(train, val) = get_iterators(batch_size)\n",
    "mod = fit(new_sym, new_args, aux_params, train, val, batch_size, num_gpus, epochs, learning_rate)\n",
    "metric = mx.metric.Accuracy()\n",
    "mod_score = mod.score(val, metric)\n",
    "print mod_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the newly trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = model_name+'-mxnet-catsvsdogs'\n",
    "epoch = 1\n",
    "mc = mod.save_checkpoint(prefix, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model, make sure you have executed previous cells to train\n",
    "import cv2\n",
    "dshape = [('data', (1,3,224,224))]\n",
    "\n",
    "def load_model(s_fname, p_fname):\n",
    "    \"\"\"\n",
    "    Load model checkpoint from file.\n",
    "    :return: (arg_params, aux_params)\n",
    "    arg_params : dict of str to NDArray\n",
    "        Model parameter, dict of name to NDArray of net's weights.\n",
    "    aux_params : dict of str to NDArray\n",
    "        Model parameter, dict of name to NDArray of net's auxiliary states.\n",
    "    \"\"\"\n",
    "    symbol = mx.symbol.load(s_fname)\n",
    "    save_dict = mx.nd.load(p_fname)\n",
    "    arg_params = {}\n",
    "    aux_params = {}\n",
    "    for k, v in save_dict.items():\n",
    "        tp, name = k.split(':', 1)\n",
    "        if tp == 'arg':\n",
    "            arg_params[name] = v\n",
    "        if tp == 'aux':\n",
    "            aux_params[name] = v\n",
    "    return symbol, arg_params, aux_params\n",
    "\n",
    "model_symbol = model_name+'-mxnet-catsvsdogs-symbol.json'\n",
    "model_params = model_name+'-mxnet-catsvsdogs-0001.params'\n",
    "sym, arg_params, aux_params = load_model(model_symbol, model_params)\n",
    "mod = mx.mod.Module(symbol=sym, context=mx.gpu())\n",
    "mod.predict\n",
    "# bind the model and set training == False; Define the data shape\n",
    "mod.bind(for_training=False, data_shapes=dshape)\n",
    "mod.set_params(arg_params, aux_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for an arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "Batch = namedtuple('Batch', ['data'])\n",
    "\n",
    "def preprocess_image(img, show_img=False):\n",
    "    '''\n",
    "    convert the image to a numpy array\n",
    "    '''\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2) \n",
    "    img = img[np.newaxis, :] \n",
    "    return img\n",
    "\n",
    "url = 'http://www.hillspet.com/HillsPetUS/v1/portal/en/us/cat-care/images/HP_PCC_md_0130_cat53.jpg'\n",
    "req = urllib2.urlopen(url)\n",
    "\n",
    "image = np.asarray(bytearray(req.read()), dtype=\"uint8\")\n",
    "image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "img = preprocess_image(image)\n",
    "\n",
    "mod.forward(Batch([mx.nd.array(img)]))\n",
    "\n",
    "# predict\n",
    "prob = mod.get_outputs()[0].asnumpy()\n",
    "print prob\n",
    "result = np.argmax(prob)\n",
    "outstring = ['Dog', 'Cat']\n",
    "print(outstring[result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions for an arbitrary image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "Batch = namedtuple('Batch', ['data'])\n",
    "\n",
    "def preprocess_image(img, show_img=False):\n",
    "    '''\n",
    "    convert the image to a numpy array\n",
    "    '''\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.swapaxes(img, 0, 2)\n",
    "    img = np.swapaxes(img, 1, 2) \n",
    "    img = img[np.newaxis, :] \n",
    "    return img\n",
    "\n",
    "def predict(url, mod):\n",
    "    req = urllib2.urlopen(url)\n",
    "\n",
    "    image = np.asarray(bytearray(req.read()), dtype=\"uint8\")\n",
    "    image = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
    "    img = preprocess_image(image)\n",
    "    plt.subplot(1, 1, 1)\n",
    "    plt.imshow(image)\n",
    "    mod.forward(Batch([mx.nd.array(img)]))\n",
    "\n",
    "    # predict\n",
    "    prob = mod.get_outputs()[0].asnumpy()\n",
    "    result = np.argmax(prob)\n",
    "    outstring = ['Cat', 'Dog']\n",
    "    return outstring[result], prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('https://www.what-dog.net/Images/faces2/scroll0015.jpg', mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('https://www.catprotection.com.au/wp-content/uploads/2014/11/1866855-cat-m-1030x686.jpg', mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('http://www.hillspet.com/HillsPetUS/v1/portal/en/us/cat-care/images/HP_PCC_md_0130_cat53.jpg', mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert MXNet to CoreML model\n",
    "\n",
    "With Core ML, you can integrate trained machine learning models into your app.\n",
    "\n",
    "\n",
    "A trained model is the result of applying a machine learning algorithm to a set of training data. The model makes predictions based on new input data. For example, a model that's been trained on a region's historical house prices may be able to predict a house's price when given the number of bedrooms and bathrooms.\n",
    "Core ML is the foundation for domain-specific frameworks and functionality. Core ML supports Vision for image analysis, Foundation for natural language processing (for example, the NSLinguisticTagger class), and GameplayKit for evaluating learned decision trees. Core ML itself builds on top of low-level primitives like Accelerate and BNNS, as well as Metal Performance Shaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src/mxnet/tools/coreml\")\n",
    "sys.path.append(\"../src/mxnet/tools/coreml/converter/\")\n",
    "from converter._mxnet_converter import convert\n",
    "from converter.utils import load_model\n",
    "\n",
    "input_shape=(1,3,224,224)\n",
    "coreml_model = convert(mod, input_shape={'data': input_shape}, preprocessor_args={\"image_input_names\":\"data\"})\n",
    "coreml_model.save('coreml.mlmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save coreml model to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.save('coreml.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
